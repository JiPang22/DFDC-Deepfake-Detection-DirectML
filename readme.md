# rPPG-based Deepfake Detection

This project utilizes a dual-branch deep learning model to detect deepfake videos by analyzing both spatial and temporal (frequency) information. The core idea is to leverage remote photoplethysmography (rPPG) signals, which capture subtle color changes in the skin caused by blood flow, as a liveness indicator.

## Key Features

- **Dual-Branch Architecture**:
  - **Spatial Branch (CNN)**: Analyzes each frame for visual artifacts, compression errors, and unnatural textures.
  - **Frequency Branch (FFT)**: Extracts the rPPG signal from the video, performs a Fast Fourier Transform (FFT), and analyzes the frequency spectrum for a periodic heartbeat signal.
- **Focal Loss**: Focuses the training on hard-to-classify examples, improving model accuracy on ambiguous videos.
- **Advanced Augmentation**: Simulates real-world conditions like poor lighting, low bit-rate, and camera noise to build a robust model.
- **Automated Pipeline**: Includes scripts for training, evaluation, and visualization, all connected in a seamless workflow.

## Performance

The model achieves an accuracy of **~68%** on the validation set. The performance is balanced across both 'Real' and 'Fake' classes, as shown in the ROC curve and confusion matrix below.

![Performance Report](results/performance_report.png)

## How to Use

### 1. Setup

Clone the repository and install the required dependencies:

```bash
git clone <repository-url>
cd Deepfake_project
pip install -r requirements.txt
```

### 2. Data Preprocessing

Place your video data (or image sequences) in the `train_data_85/real` and `train_data_85/fake` directories. Then, run the preprocessing script to convert them into `.npy` tensors.

```bash
python src/preprocess_from_images.py
```

### 3. Training

Run the training script. The best model will be saved as `src/rppg_best_real.pth`.

```bash
# For a quick 30-minute run
python src/train_quick.py

# For a full, in-depth training session
python src/train_final.py
```

### 4. Evaluation & Inference

After training, the script will automatically evaluate the model and run a random inference test. You can also run them manually:

```bash
# Evaluate the best model
python src/evaluate.py

# Test with a random sample
python src/inference.py
```
# ğŸ›¡ï¸ Deepfake Detection with rPPG & Hybrid ViViT

ì´ í”„ë¡œì íŠ¸ëŠ” ì‹œê°ì  ì™œê³¡ë¿ë§Œ ì•„ë‹ˆë¼ ìƒì²´ ì‹ í˜¸(rPPG)ì˜ ì¼ê´€ì„±ì„ ë¶„ì„í•˜ì—¬ ë”¥í˜ì´í¬ ì—¬ë¶€ë¥¼ íŒë³„í•˜ëŠ” ê³ ì„±ëŠ¥ AI ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤. AMD ROCm í™˜ê²½(RX 6600)ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ“‚ Project Structure
```text
Deepfake_project/
â”œâ”€â”€ data/               # (Git ì œì™¸) ì›ë³¸ ë° ì „ì²˜ë¦¬ ë°ì´í„°ì…‹
â”œâ”€â”€ models/             # (Git ì œì™¸) í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ (.pth)
â”œâ”€â”€ src/                # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ model.py        # ViViT + rPPG í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì •ì˜
â”‚   â”œâ”€â”€ train.py        # ì‹œê°í™”(plt) ë° ì¡°ê¸° ì¢…ë£Œ ê¸°ëŠ¥ í¬í•¨ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ inference.py    # ë¹„ë””ì˜¤ ì „ì²´ ë¶„ì„ ìŠ¤ìºë„ˆ
â”œâ”€â”€ results/            # í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ë“± ê²°ê³¼ë¬¼
â”œâ”€â”€ .gitignore          # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë° ë°”ì´ë„ˆë¦¬ ì œì™¸ ì„¤ì •
â””â”€â”€ README.md           # í”„ë¡œì íŠ¸ ê°€ì´ë“œ


## ğŸ› ï¸ Execution Guide (ì‹¤í–‰ ìˆœì„œ)

í”„ë¡œì íŠ¸ë¥¼ ì²˜ìŒ ì‹œì‘í•  ë•Œ ë‹¤ìŒ ìˆœì„œëŒ€ë¡œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹­ì‹œì˜¤.

### 1. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬

ë¨¼ì € ì›ë³¸ ì˜ìƒì—ì„œ í•™ìŠµì— í•„ìš”í•œ ì–¼êµ´ í”„ë ˆì„ì„ ê³ ì†ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.

```bash
# DFDC ì˜ìƒì—ì„œ 10í”„ë ˆì„ì”© ì¶”ì¶œí•˜ì—¬ data/processed/ì— ì €ì¥
python src/extract_frames.py

```

### 2. ëª¨ë¸ í•™ìŠµ (rPPG + ViViT)

ìƒì²´ ì‹ í˜¸ì™€ ì‹œê°„ì  íë¦„ì„ ë¶„ì„í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

```bash
# í•™ìŠµ ì§„í–‰, ETA ì¶œë ¥, 0.5% ì˜¤ì°¨ ì‹œ ì¡°ê¸° ì¢…ë£Œ ë° ê²°ê³¼ ê·¸ë˜í”„ íŒì—…
python src/train.py

```

### 3. ì˜ìƒ íŒë³„ (Inference)

í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì˜ìƒ íŒŒì¼(.mp4)ì˜ ìœ„ì¡° ì—¬ë¶€ë¥¼ ìŠ¤ìº”í•©ë‹ˆë‹¤.

```bash
# ì˜ìƒ ë‚´ 30ê°œ í”„ë ˆì„ì„ ì¢…í•© ë¶„ì„í•˜ì—¬ ìµœì¢… í™•ë¥  ì¶œë ¥
python src/inference.py

```

## ğŸš€ Key Technologies

* **rPPG (remote Photoplethysmography)**: ì–¼êµ´ í”¼ë¶€ì˜ ë¯¸ì„¸í•œ ìƒ‰ìƒ ë³€í™”ë¥¼ ì¶”ì í•˜ì—¬ ì‹¬ë°• ì‹ í˜¸ì˜ ìœ ë¬´ë¥¼ íŒë³„í•©ë‹ˆë‹¤.
* **Hybrid ViViT**: CNN(ê³µê°„ ë¶„ì„) + LSTM(ì‹œê°„ ë¶„ì„) êµ¬ì¡°ë¡œ ë¹„ë””ì˜¤ ë°ì´í„°ì— ìµœì í™”ëœ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* **Smart Training**: Train/Val Accuracy ì˜¤ì°¨ê°€ 0.5% ì´ë‚´ ìˆ˜ë ´ ì‹œ ìë™ ì €ì¥ ë° ì¢…ë£Œ.

## ğŸ› ï¸ Environment

* **GPU**: AMD Radeon RX 6600 (ROCm 10.3.0 Override ì ìš©)
* **OS**: Linux (Ubuntu 22.04/24.04 ê¶Œì¥)

```

---

### ğŸ“‹ ì§€ì‹ ë§¤í•‘
* **Execution Flow**: AI í”„ë¡œì íŠ¸ì—ì„œ ì „ì²˜ë¦¬(Pre-processing) â†’ í•™ìŠµ(Training) â†’ ì¶”ë¡ (Inference) ìˆœì„œë¥¼ ëª…ì‹œí•˜ëŠ” ê²ƒì€ ì½”ë“œ ì‚¬ìš©ìì˜ ê°€ë…ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
* **Reproducibility**: `README.md`_ì— ëª…ì‹œëœ ëª…ë ¹ì–´ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¼ í–ˆì„ ë•Œ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œ í•˜ëŠ” ê²ƒì´ ë°°í¬ì˜ í•µì‹¬ì…ë‹ˆë‹¤.

