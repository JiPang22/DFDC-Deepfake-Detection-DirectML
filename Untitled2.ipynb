{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18cede0e-8592-4a13-97cb-65c291f67ac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msolutions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m face_detection \u001b[38;5;28;01mas\u001b[39;00m mp_face_detection\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 2. MediaPipe ì–¼êµ´ íƒì§€ê¸° ì„¤ì •\u001b[39;00m\n\u001b[32m      5\u001b[39m face_detector = mp_face_detection.FaceDetection(model_selection=\u001b[32m1\u001b[39m, min_detection_confidence=\u001b[32m0.5\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mediapipe.python'"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.python.solutions import face_detection as mp_face_detection\n",
    "\n",
    "# 2. MediaPipe ì–¼êµ´ íƒì§€ê¸° ì„¤ì •\n",
    "face_detector = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00b8098-6743-4805-a5e4-4726d2fc7973",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'real_videos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m         cap.release()\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# 3. ì‹¤í–‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m quick_extract_opencv(\u001b[43mreal_videos\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mREAL\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     38\u001b[39m quick_extract_opencv(fake_videos, \u001b[33m'\u001b[39m\u001b[33mFAKE\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… ì™„ë£Œ! \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m í´ë”ì— ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'real_videos' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì •\n",
    "save_base = '/mnt/c/Users/wlvud/dfdc_test_faces'\n",
    "os.makedirs(save_base, exist_ok=True)\n",
    "\n",
    "# 2. OpenCV ê¸°ë³¸ ì–¼êµ´ íƒì§€ê¸° (Haar Cascade) ë¡œë“œ\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def quick_extract_opencv(video_list, label):\n",
    "    save_dir = os.path.join(save_base, label)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for v_path in tqdm(video_list, desc=f\"Extracting {label}\"):\n",
    "        cap = cv2.VideoCapture(v_path)\n",
    "        v_name = os.path.basename(v_path)\n",
    "        \n",
    "        for i in range(3): # 3í”„ë ˆì„ ì¶”ì¶œ\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i * 30)\n",
    "            success, frame = cap.read()\n",
    "            if not success: break\n",
    "            \n",
    "            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜ (íƒì§€ ì†ë„ í–¥ìƒ)\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "                face = cv2.resize(face, (224, 224))\n",
    "                cv2.imwrite(f\"{save_dir}/{v_name}_f{i}.jpg\", face)\n",
    "                break # ì²« ë²ˆì§¸ ì–¼êµ´ë§Œ ì €ì¥\n",
    "        cap.release()\n",
    "\n",
    "# 3. ì‹¤í–‰\n",
    "quick_extract_opencv(real_videos, 'REAL')\n",
    "quick_extract_opencv(fake_videos, 'FAKE')\n",
    "\n",
    "print(f\"\\nâœ… ì™„ë£Œ! {save_base} í´ë”ì— ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e749cbde-6cfe-4f06-b7ef-cff74d5cff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing REAL:   0%|                                       | 0/50 [00:00<?, ?it/s]pid:11907 tid:0x7684a8105080 [topology_sysfs_get_system_props] No WDDM adapters found.\n",
      "Processing REAL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:16<00:00,  3.04it/s]\n",
      "Processing FAKE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:16<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ì™„ë£Œ! /mnt/c/Users/wlvud/dfdc_test_faces í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. ê²½ë¡œ ì¬ì„¤ì • (ì‚¬ìš©ì í™˜ê²½ ë§ì¶¤)\n",
    "base_path = '/mnt/c/Users/wlvud/dfdc_data'\n",
    "save_base = '/mnt/c/Users/wlvud/dfdc_test_faces'\n",
    "os.makedirs(save_base, exist_ok=True)\n",
    "\n",
    "# 2. ëª¨ë“  metadata.json í†µí•© ë¡œë“œ\n",
    "json_files = glob.glob(os.path.join(base_path, '**/metadata.json'), recursive=True)\n",
    "all_metadata = {}\n",
    "for j_path in json_files:\n",
    "    root_dir = os.path.dirname(j_path)\n",
    "    with open(j_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for filename, info in data.items():\n",
    "            all_metadata[os.path.join(root_dir, filename)] = info\n",
    "\n",
    "# 3. REAL/FAKE ë¦¬ìŠ¤íŠ¸ ì„ ë³„\n",
    "real_videos = [path for path, info in all_metadata.items() if info['label'] == 'REAL'][:50]\n",
    "fake_videos = [path for path, info in all_metadata.items() if info['label'] == 'FAKE'][:50]\n",
    "test_targets = [('REAL', real_videos), ('FAKE', fake_videos)]\n",
    "\n",
    "# 4. OpenCV íƒì§€ê¸° ë° ì¶”ì¶œ í•¨ìˆ˜\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def quick_extract_integrated():\n",
    "    for label, v_list in test_targets:\n",
    "        save_dir = os.path.join(save_base, label)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        for v_path in tqdm(v_list, desc=f\"Processing {label}\"):\n",
    "            cap = cv2.VideoCapture(v_path)\n",
    "            v_name = os.path.basename(v_path)\n",
    "            \n",
    "            for i in range(3): # ì˜ìƒë‹¹ 3í”„ë ˆì„\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, i * 30)\n",
    "                success, frame = cap.read()\n",
    "                if not success: break\n",
    "                \n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "                \n",
    "                for (x, y, w, h) in faces:\n",
    "                    face = frame[y:y+h, x:x+w]\n",
    "                    face = cv2.resize(face, (224, 224))\n",
    "                    cv2.imwrite(os.path.join(save_dir, f\"{v_name}_f{i}.jpg\"), face)\n",
    "                    break\n",
    "            cap.release()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "quick_extract_integrated()\n",
    "print(f\"\\nâœ… ì™„ë£Œ! {save_base} í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92bb5d9e-fa50-4ffd-93af-7d02e14a5bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropped Escape call with ulEscapeCode : 0x03007703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RX 6600 í•™ìŠµ ë£¨í”„ ì§„ì…...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpjp/dfdc_env/lib/python3.12/site-packages/torch/nn/functional.py:3172: UserWarning: The operator 'aten::binary_cross_entropy' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /__w/1/s/pytorch-directml-plugin/torch_directml/csrc/dml/dml_cpu_fallback.cpp:15.)\n",
      "  return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.9064 | Accuracy: 53.81%\n",
      "Epoch 2 | Loss: 0.8617 | Accuracy: 52.38%\n",
      "Epoch 3 | Loss: 1.8588 | Accuracy: 59.52%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 1. DirectML ì¥ì¹˜ í™œì„±í™”\n",
    "device = torch_directml.device()\n",
    "\n",
    "# 2. ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ì •ì˜\n",
    "class DFDCFaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for label in ['REAL', 'FAKE']:\n",
    "            path = os.path.join(root_dir, label)\n",
    "            for img in os.listdir(path):\n",
    "                self.samples.append((os.path.join(path, img), 1 if label == 'FAKE' else 0))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "# 3. ëª¨ë¸ ë° ì„¤ì • (Binary Classification)\n",
    "transform = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
    "dataset = DFDCFaceDataset('/mnt/c/Users/wlvud/dfdc_test_faces', transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 30 * 30, 1), nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 4. 1ë¶„ ë‚´ì™¸ í•™ìŠµ ë£¨í”„\n",
    "print(\"RX 6600 í•™ìŠµ ë£¨í”„ ì§„ì…...\")\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = (correct / total) * 100\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f} | Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb219bf8-3aa3-4c69-93ca-474875e42174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RX 6600 (DirectML) ìµœì í™” í•™ìŠµ ì‹œì‘...\n",
      "Epoch 1 | Loss: 0.6721 | Acc: 57.14%\n",
      "Epoch 2 | Loss: 0.6706 | Acc: 55.71%\n",
      "Epoch 3 | Loss: 0.6314 | Acc: 64.76%\n",
      "Epoch 4 | Loss: 0.6073 | Acc: 67.62%\n",
      "Epoch 5 | Loss: 0.5956 | Acc: 68.10%\n",
      "ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 1. DirectML ì¥ì¹˜ í™œì„±í™”\n",
    "device = torch_directml.device()\n",
    "\n",
    "# 2. ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ì •ì˜\n",
    "class DFDCFaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for label in ['REAL', 'FAKE']:\n",
    "            path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(path): continue\n",
    "            for img in os.listdir(path):\n",
    "                self.samples.append((os.path.join(path, img), 1 if label == 'FAKE' else 0))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "# 3. ë°ì´í„° ë¡œë” (128x128 ë¦¬ì‚¬ì´ì¦ˆ)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "dataset = DFDCFaceDataset('/mnt/c/Users/wlvud/dfdc_test_faces', transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 4. ëª¨ë¸ ì„¤ê³„ (BCEWithLogitsLoss ëŒ€ì‘ì„ ìœ„í•´ ë§ˆì§€ë§‰ Sigmoid ì œê±°)\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128 * 16 * 16, 1) \n",
    ").to(device)\n",
    "\n",
    "# 5. ìµœì í™” ë„êµ¬: ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•´ LogitsLoss ì‚¬ìš©\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 6. í•™ìŠµ ë£¨í”„\n",
    "print(\"RX 6600 (DirectML) ìµœì í™” í•™ìŠµ ì‹œì‘...\")\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ì •í™•ë„ ê³„ì‚° (Logitì„ Sigmoid í™•ë¥ ê°’ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ íŒì •)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    acc = (correct / total) * 100\n",
    "    print(f\"Epoch {epoch+1} | Loss: {running_loss/len(loader):.4f} | Acc: {acc:.2f}%\")\n",
    "\n",
    "print(\"ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684239e9-4c9c-418c-9f8d-c52c74f42d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì´ì œ ì ˆë°˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c57c4f1-6b2a-4807-8dd5-d09041789405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 0ê°œ ì˜ìƒ(50%) ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (Workers: 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì ˆë°˜ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ. ì €ì¥ ê²½ë¡œ: /mnt/c/Users/wlvud/dfdc_full_faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. ì„¤ì •\n",
    "DATA_ROOT = \"/mnt/c/Users/wlvud/dfdc_data\"\n",
    "SAVE_ROOT = \"/mnt/c/Users/wlvud/dfdc_full_faces\"\n",
    "MAX_WORKERS = 10 \n",
    "\n",
    "os.makedirs(os.path.join(SAVE_ROOT, 'REAL'), exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_ROOT, 'FAKE'), exist_ok=True)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def process_video(video_info):\n",
    "    video_path, label = video_info\n",
    "    video_name = os.path.basename(video_path).split('.')[0]\n",
    "    save_path = os.path.join(SAVE_ROOT, label, f\"{video_name}.jpg\")\n",
    "    \n",
    "    if os.path.exists(save_path): return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_img = cv2.resize(face_img, (256, 256))\n",
    "            cv2.imwrite(save_path, face_img)\n",
    "            break\n",
    "    cap.release()\n",
    "\n",
    "def get_half_video_list():\n",
    "    all_videos = []\n",
    "    for i in range(10):\n",
    "        part_path = os.path.join(DATA_ROOT, f\"dfdc_train_part_{i}\")\n",
    "        meta_path = os.path.join(part_path, \"metadata.json\")\n",
    "        if os.path.exists(meta_path):\n",
    "            with open(meta_path, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "                for v_name, info in meta.items():\n",
    "                    all_videos.append((os.path.join(part_path, v_name), info['label']))\n",
    "    \n",
    "    # ì ˆë°˜ë§Œ ëœë¤ ìƒ˜í”Œë§\n",
    "    random.shuffle(all_videos)\n",
    "    return all_videos[:len(all_videos)//2]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    videos = get_half_video_list()\n",
    "    print(f\"ì´ {len(videos)}ê°œ ì˜ìƒ(50%) ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (Workers: {MAX_WORKERS})\")\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        list(tqdm(executor.map(process_video, videos), total=len(videos)))\n",
    "\n",
    "print(f\"ì ˆë°˜ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ. ì €ì¥ ê²½ë¡œ: {SAVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5182617a-b452-4ef8-a923-dfc5a4a8e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ì˜ìƒì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: /mnt/c/Users/wlvud/dfdc_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. ì„¤ì • (ê²½ë¡œ ë‹¤ì‹œ í™•ì¸)\n",
    "DATA_ROOT = \"/mnt/c/Users/wlvud/dfdc_data\"\n",
    "SAVE_ROOT = \"/mnt/c/Users/wlvud/dfdc_full_faces\"\n",
    "MAX_WORKERS = 10 \n",
    "\n",
    "os.makedirs(os.path.join(SAVE_ROOT, 'REAL'), exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_ROOT, 'FAKE'), exist_ok=True)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def process_video(video_info):\n",
    "    video_path, label = video_info\n",
    "    video_name = os.path.basename(video_path).split('.')[0]\n",
    "    save_path = os.path.join(SAVE_ROOT, label, f\"{video_name}.jpg\")\n",
    "    \n",
    "    if os.path.exists(save_path): return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_img = cv2.resize(face_img, (256, 256))\n",
    "            cv2.imwrite(save_path, face_img)\n",
    "            break\n",
    "    cap.release()\n",
    "\n",
    "def get_half_video_list():\n",
    "    all_videos = []\n",
    "    # í´ë” êµ¬ì¡°ê°€ dfdc_data/dfdc_train_part_X/metadata.json ì¸ì§€ í™•ì¸\n",
    "    for part_folder in os.listdir(DATA_ROOT):\n",
    "        part_path = os.path.join(DATA_ROOT, part_folder)\n",
    "        if not os.path.isdir(part_path): continue\n",
    "        \n",
    "        meta_path = os.path.join(part_path, \"metadata.json\")\n",
    "        if os.path.exists(meta_path):\n",
    "            with open(meta_path, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "                for v_name, info in meta.items():\n",
    "                    # ì‹¤ì œ ì˜ìƒ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ í›„ ì¶”ê°€\n",
    "                    v_full_path = os.path.join(part_path, v_name)\n",
    "                    if os.path.exists(v_full_path):\n",
    "                        all_videos.append((v_full_path, info['label']))\n",
    "    \n",
    "    random.shuffle(all_videos)\n",
    "    return all_videos[:len(all_videos)//2]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    videos = get_half_video_list()\n",
    "    if not videos:\n",
    "        print(f\"âŒ ì˜ìƒì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {DATA_ROOT}\")\n",
    "    else:\n",
    "        print(f\"âœ… ì´ {len(videos)}ê°œ ì˜ìƒ(50%) ì¶”ì¶œ ì‹œì‘...\")\n",
    "        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            list(tqdm(executor.map(process_video, videos), total=len(videos)))\n",
    "        print(f\"ì „ì²˜ë¦¬ ì™„ë£Œ: {SAVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ead6450-9d85-4396-870b-7b15736f8e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ì˜ìƒì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œ ë‚´ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì • (í™•ì¸ëœ 00~09 í¬ë§· ë°˜ì˜)\n",
    "DATA_ROOT = \"/mnt/c/Users/wlvud/dfdc_data\"\n",
    "SAVE_ROOT = \"/mnt/c/Users/wlvud/dfdc_full_faces\"\n",
    "MAX_WORKERS = 10 \n",
    "\n",
    "os.makedirs(os.path.join(SAVE_ROOT, 'REAL'), exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_ROOT, 'FAKE'), exist_ok=True)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def process_video(video_info):\n",
    "    video_path, label = video_info\n",
    "    video_name = os.path.basename(video_path).split('.')[0]\n",
    "    save_path = os.path.join(SAVE_ROOT, label, f\"{video_name}.jpg\")\n",
    "    \n",
    "    if os.path.exists(save_path): return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # ì†ë„ í–¥ìƒì„ ìœ„í•œ detectMultiScale íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_img = cv2.resize(face_img, (256, 256))\n",
    "            cv2.imwrite(save_path, face_img)\n",
    "            break\n",
    "    cap.release()\n",
    "\n",
    "def get_half_video_list():\n",
    "    all_videos = []\n",
    "    # dfdc_train_part_00 ë¶€í„° 09 ê¹Œì§€ íƒìƒ‰\n",
    "    for i in range(10):\n",
    "        part_name = f\"dfdc_train_part_{i:02d}\" # 0 -> 00, 1 -> 01 ë¡œ ë³€í™˜\n",
    "        part_path = os.path.join(DATA_ROOT, part_name)\n",
    "        \n",
    "        meta_path = os.path.join(part_path, \"metadata.json\")\n",
    "        if os.path.exists(meta_path):\n",
    "            with open(meta_path, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "                for v_name, info in meta.items():\n",
    "                    v_full_path = os.path.join(part_path, v_name)\n",
    "                    if os.path.exists(v_full_path):\n",
    "                        all_videos.append((v_full_path, info['label']))\n",
    "    \n",
    "    random.shuffle(all_videos)\n",
    "    return all_videos[:len(all_videos)//2]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    videos = get_half_video_list()\n",
    "    if not videos:\n",
    "        print(f\"âŒ ì˜ìƒì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œ ë‚´ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    else:\n",
    "        print(f\"âœ… ì´ {len(videos)}ê°œ ì˜ìƒ(50%) ì¶”ì¶œ ì‹œì‘ (Workers: {MAX_WORKERS})\")\n",
    "        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            list(tqdm(executor.map(process_video, videos), total=len(videos)))\n",
    "        print(f\"ì „ì²˜ë¦¬ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {SAVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b2604f0-2dac-4d9a-a04d-9e81af230616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” /mnt/c/Users/wlvud/dfdc_dataì—ì„œ ë°ì´í„°ë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\n",
      "âœ… ì´ 9954ê°œ ì˜ìƒ(50%) ì¶”ì¶œ ì‹œì‘ (Workers: 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9954/9954 [06:38<00:00, 24.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: /mnt/c/Users/wlvud/dfdc_full_faces\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì •\n",
    "DATA_ROOT = \"/mnt/c/Users/wlvud/dfdc_data\"\n",
    "SAVE_ROOT = \"/mnt/c/Users/wlvud/dfdc_full_faces\"\n",
    "MAX_WORKERS = 10 \n",
    "\n",
    "os.makedirs(os.path.join(SAVE_ROOT, 'REAL'), exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_ROOT, 'FAKE'), exist_ok=True)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def process_video(video_info):\n",
    "    video_path, label = video_info\n",
    "    video_name = os.path.basename(video_path).split('.')[0]\n",
    "    save_path = os.path.join(SAVE_ROOT, label, f\"{video_name}.jpg\")\n",
    "    \n",
    "    if os.path.exists(save_path): return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_img = cv2.resize(face_img, (256, 256))\n",
    "            cv2.imwrite(save_path, face_img)\n",
    "            break\n",
    "    cap.release()\n",
    "\n",
    "def get_half_video_list():\n",
    "    all_videos = []\n",
    "    print(f\"ğŸ” {DATA_ROOT}ì—ì„œ ë°ì´í„°ë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\")\n",
    "    \n",
    "    # os.walkë¡œ ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ ì „ìˆ˜ ì¡°ì‚¬\n",
    "    for root, dirs, files in os.walk(DATA_ROOT):\n",
    "        if \"metadata.json\" in files:\n",
    "            meta_path = os.path.join(root, \"metadata.json\")\n",
    "            try:\n",
    "                with open(meta_path, 'r') as f:\n",
    "                    meta = json.load(f)\n",
    "                    for v_name, info in meta.items():\n",
    "                        v_full_path = os.path.join(root, v_name)\n",
    "                        # íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•  ë•Œë§Œ ì¶”ê°€\n",
    "                        if os.path.isfile(v_full_path):\n",
    "                            all_videos.append((v_full_path, info['label']))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ {meta_path} ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "                continue\n",
    "    \n",
    "    random.shuffle(all_videos)\n",
    "    return all_videos[:len(all_videos)//2]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    videos = get_half_video_list()\n",
    "    if not videos:\n",
    "        print(f\"âŒ ì—¬ì „íˆ ì˜ìƒì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"í˜„ì¬ {DATA_ROOT}ì˜ í•˜ìœ„ ëª©ë¡: {os.listdir(DATA_ROOT)}\")\n",
    "    else:\n",
    "        print(f\"âœ… ì´ {len(videos)}ê°œ ì˜ìƒ(50%) ì¶”ì¶œ ì‹œì‘ (Workers: {MAX_WORKERS})\")\n",
    "        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            list(tqdm(executor.map(process_video, videos), total=len(videos)))\n",
    "        print(f\"ì „ì²˜ë¦¬ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {SAVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a65f32-2e9e-4883-a158-75879e5f8887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 ì‹¤ì „ í•™ìŠµ ì‹œì‘ (Train: 5168, Val: 1292)\n",
      "Epoch 1/10 | Loss: 0.4218 | Val Acc: 86.84%\n",
      "Epoch 2/10 | Loss: 0.4127 | Val Acc: 86.84%\n",
      "Epoch 3/10 | Loss: 0.4064 | Val Acc: 86.84%\n",
      "Epoch 4/10 | Loss: 0.4010 | Val Acc: 86.84%\n",
      "Epoch 5/10 | Loss: 0.4007 | Val Acc: 86.84%\n",
      "Epoch 6/10 | Loss: 0.3973 | Val Acc: 86.84%\n",
      "Epoch 7/10 | Loss: 0.3958 | Val Acc: 86.76%\n",
      "Epoch 8/10 | Loss: 0.3951 | Val Acc: 86.84%\n",
      "Epoch 9/10 | Loss: 0.3929 | Val Acc: 85.06%\n",
      "Epoch 10/10 | Loss: 0.3851 | Val Acc: 86.84%\n",
      "í•™ìŠµ ì™„ë£Œ. ëª¨ë¸ì„ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 1. DirectML ì„¤ì •\n",
    "device = torch_directml.device()\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "class DFDCFullDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for label in ['REAL', 'FAKE']:\n",
    "            path = os.path.join(root_dir, label)\n",
    "            for img in os.listdir(path):\n",
    "                self.samples.append((os.path.join(path, img), 1 if label == 'FAKE' else 0))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "# 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í•  (8:2)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # ResNet í‘œì¤€ ì…ë ¥ í¬ê¸°\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = DFDCFullDataset('/mnt/c/Users/wlvud/dfdc_full_faces', transform=transform)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_db, val_db = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_db, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_db, batch_size=32, shuffle=False)\n",
    "\n",
    "# 4. ResNet18 ëª¨ë¸ ì»¤ìŠ¤í…€\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1) # ìµœì¢… ì¶œë ¥ì„ 1ê°œ(Sigmoidìš©)ë¡œ ë³€ê²½\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. í•™ìŠµ ë£¨í”„\n",
    "print(f\"ResNet18 ì‹¤ì „ í•™ìŠµ ì‹œì‘ (Train: {train_size}, Val: {val_size})\")\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # ê²€ì¦ ë£¨í”„\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/10 | Loss: {train_loss/len(train_loader):.4f} | Val Acc: {100*correct/total:.2f}%\")\n",
    "\n",
    "print(\"í•™ìŠµ ì™„ë£Œ. ëª¨ë¸ì„ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db0c63c1-038d-4441-bbaf-639782acd8b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset, WeightedRandomSampler\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms, models\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchsummary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. DirectML ì„¤ì •\n",
    "device = torch_directml.device()\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ ë° ê°€ì¤‘ì¹˜ ìƒ˜í”ŒëŸ¬ (ë¶ˆê· í˜• í•´ê²°)\n",
    "class DFDCFinalDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        for label in ['REAL', 'FAKE']:\n",
    "            path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(path): continue\n",
    "            for img in os.listdir(path):\n",
    "                l_idx = 1 if label == 'FAKE' else 0\n",
    "                self.samples.append((os.path.join(path, img), l_idx))\n",
    "                self.labels.append(l_idx)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = DFDCFinalDataset('/mnt/c/Users/wlvud/dfdc_full_faces', transform=transform)\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ìƒ˜í”ŒëŸ¬: REAL ë°ì´í„°ê°€ í•™ìŠµì— ë” ìì£¼ ë…¸ì¶œë˜ë„ë¡ ì„¤ì •\n",
    "class_counts = np.bincount(dataset.labels)\n",
    "weights = 1. / class_counts\n",
    "samples_weights = torch.from_numpy(weights[dataset.labels])\n",
    "sampler = WeightedRandomSampler(samples_weights, len(samples_weights))\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "# 3. ëª¨ë¸ ì •ì˜ ë° ìš”ì•½í‘œ ì¶œë ¥\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"     MODEL ARCHITECTURE\")\n",
    "print(\"=\"*30)\n",
    "summary(model, input_size=(3, 224, 224), device=\"cpu\")\n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "# 4. í•™ìŠµ ì„¤ì •\n",
    "total_epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 5. ì‹œê°„ ì¸¡ì • í•™ìŠµ ë£¨í”„\n",
    "start_time = time.time()\n",
    "print(f\"í•™ìŠµ ì‹œì‘ ì‹œê°: {time.ctime(start_time)}\")\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # ì—í¬í¬ ì‹œê°„ ê³„ì‚°\n",
    "    epoch_end = time.time()\n",
    "    epoch_duration = epoch_end - epoch_start\n",
    "    total_elapsed = epoch_end - start_time\n",
    "    \n",
    "    # ë‚¨ì€ ì‹œê°„ ì˜ˆì¸¡\n",
    "    avg_epoch_time = total_elapsed / (epoch + 1)\n",
    "    remaining_epochs = total_epochs - (epoch + 1)\n",
    "    estimated_remaining_time = avg_epoch_time * remaining_epochs\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}] ì™„ë£Œ\")\n",
    "    print(f\" - ì†Œìš” ì‹œê°„: {timedelta(seconds=int(epoch_duration))}\")\n",
    "    print(f\" - í˜„ì¬ê¹Œì§€ ì´ ê²½ê³¼ ì‹œê°„: {timedelta(seconds=int(total_elapsed))}\")\n",
    "    print(f\" - ì˜ˆìƒ ì¢…ë£Œ ì‹œê°ê¹Œì§€ ë‚¨ì€ ì‹œê°„: {timedelta(seconds=int(estimated_remaining_time))}\")\n",
    "    print(f\" - í‰ê·  Loss: {running_loss/len(loader):.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "total_end_time = time.time()\n",
    "print(f\"\\nìµœì¢… í•™ìŠµ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {timedelta(seconds=int(total_end_time - start_time))}\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "torch.save(model.state_dict(), \"dfdc_resnet18_final.pth\")\n",
    "print(\"ì €ì¥ ì™„ë£Œ: dfdc_resnet18_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e92c7a-1544-41cc-bbb1-f97e9b9d6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ff7413-68b9-4d2f-b958-df0ada91d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropped Escape call with ulEscapeCode : 0x03007703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "     MODEL ARCHITECTURE\n",
      "==============================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (privateuseoneFloatType) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m     MODEL ARCHITECTURE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# 4. í•™ìŠµ ì„¤ì •\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torchsummary/torchsummary.py:72\u001b[39m, in \u001b[36msummary\u001b[39m\u001b[34m(model, input_size, batch_size, device)\u001b[39m\n\u001b[32m     68\u001b[39m model.apply(register_hook)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torchvision/models/resnet.py:268\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m    270\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1603\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1600\u001b[39m     bw_hook = hooks.BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1601\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1603\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1604\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1605\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1606\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1607\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1608\u001b[39m     ):\n\u001b[32m   1609\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m'\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(F.pad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode),\n\u001b[32m    452\u001b[39m                     weight, bias, \u001b[38;5;28mself\u001b[39m.stride,\n\u001b[32m    453\u001b[39m                     _pair(\u001b[32m0\u001b[39m), \u001b[38;5;28mself\u001b[39m.dilation, \u001b[38;5;28mself\u001b[39m.groups)\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.FloatTensor) and weight type (privateuseoneFloatType) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. DirectML ì„¤ì •\n",
    "device = torch_directml.device()\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ ë° ê°€ì¤‘ì¹˜ ìƒ˜í”ŒëŸ¬ (ë¶ˆê· í˜• í•´ê²°)\n",
    "class DFDCFinalDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        for label in ['REAL', 'FAKE']:\n",
    "            path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(path): continue\n",
    "            for img in os.listdir(path):\n",
    "                l_idx = 1 if label == 'FAKE' else 0\n",
    "                self.samples.append((os.path.join(path, img), l_idx))\n",
    "                self.labels.append(l_idx)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = DFDCFinalDataset('/mnt/c/Users/wlvud/dfdc_full_faces', transform=transform)\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ìƒ˜í”ŒëŸ¬: REAL ë°ì´í„°ê°€ í•™ìŠµì— ë” ìì£¼ ë…¸ì¶œë˜ë„ë¡ ì„¤ì •\n",
    "class_counts = np.bincount(dataset.labels)\n",
    "weights = 1. / class_counts\n",
    "samples_weights = torch.from_numpy(weights[dataset.labels])\n",
    "sampler = WeightedRandomSampler(samples_weights, len(samples_weights))\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "# 3. ëª¨ë¸ ì •ì˜ ë° ìš”ì•½í‘œ ì¶œë ¥\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"     MODEL ARCHITECTURE\")\n",
    "print(\"=\"*30)\n",
    "summary(model, input_size=(3, 224, 224), device=\"cpu\")\n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "# 4. í•™ìŠµ ì„¤ì •\n",
    "total_epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 5. ì‹œê°„ ì¸¡ì • í•™ìŠµ ë£¨í”„\n",
    "start_time = time.time()\n",
    "print(f\"í•™ìŠµ ì‹œì‘ ì‹œê°: {time.ctime(start_time)}\")\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # ì—í¬í¬ ì‹œê°„ ê³„ì‚°\n",
    "    epoch_end = time.time()\n",
    "    epoch_duration = epoch_end - epoch_start\n",
    "    total_elapsed = epoch_end - start_time\n",
    "    \n",
    "    # ë‚¨ì€ ì‹œê°„ ì˜ˆì¸¡\n",
    "    avg_epoch_time = total_elapsed / (epoch + 1)\n",
    "    remaining_epochs = total_epochs - (epoch + 1)\n",
    "    estimated_remaining_time = avg_epoch_time * remaining_epochs\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}] ì™„ë£Œ\")\n",
    "    print(f\" - ì†Œìš” ì‹œê°„: {timedelta(seconds=int(epoch_duration))}\")\n",
    "    print(f\" - í˜„ì¬ê¹Œì§€ ì´ ê²½ê³¼ ì‹œê°„: {timedelta(seconds=int(total_elapsed))}\")\n",
    "    print(f\" - ì˜ˆìƒ ì¢…ë£Œ ì‹œê°ê¹Œì§€ ë‚¨ì€ ì‹œê°„: {timedelta(seconds=int(estimated_remaining_time))}\")\n",
    "    print(f\" - í‰ê·  Loss: {running_loss/len(loader):.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "total_end_time = time.time()\n",
    "print(f\"\\nìµœì¢… í•™ìŠµ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {timedelta(seconds=int(total_end_time - start_time))}\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "torch.save(model.state_dict(), \"dfdc_resnet18_final.pth\")\n",
    "print(\"ì €ì¥ ì™„ë£Œ: dfdc_resnet18_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34aea6d-d053-4600-9a45-6c03021f2762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "     MODEL ARCHITECTURE\n",
      "==============================\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input device is not valid, please specify 'cuda' or 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# device ì¸ìì— ì§ì ‘ ìƒì„±í•œ device ê°ì²´ ì „ë‹¬\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# 4. í•™ìŠµ ì„¤ì •\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torchsummary/torchsummary.py:45\u001b[39m, in \u001b[36msummary\u001b[39m\u001b[34m(model, input_size, batch_size, device)\u001b[39m\n\u001b[32m     42\u001b[39m         hooks.append(module.register_forward_hook(hook))\n\u001b[32m     44\u001b[39m device = device.lower()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     48\u001b[39m ], \u001b[33m\"\u001b[39m\u001b[33mInput device is not valid, please specify \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     51\u001b[39m     dtype = torch.cuda.FloatTensor\n",
      "\u001b[31mAssertionError\u001b[39m: Input device is not valid, please specify 'cuda' or 'cpu'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. DirectML ì„¤ì •\n",
    "device = torch_directml.device()\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ ë° ê°€ì¤‘ì¹˜ ìƒ˜í”ŒëŸ¬ (REAL/FAKE ë¶ˆê· í˜• í•´ì†Œ)\n",
    "class DFDCFinalDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        for label in ['REAL', 'FAKE']:\n",
    "            path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(path): continue\n",
    "            for img in os.listdir(path):\n",
    "                l_idx = 1 if label == 'FAKE' else 0\n",
    "                self.samples.append((os.path.join(path, img), l_idx))\n",
    "                self.labels.append(l_idx)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = DFDCFinalDataset('/mnt/c/Users/wlvud/dfdc_full_faces', transform=transform)\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ìƒ˜í”ŒëŸ¬ ì„¤ì •\n",
    "class_counts = np.bincount(dataset.labels)\n",
    "weights = 1. / class_counts\n",
    "samples_weights = torch.from_numpy(weights[dataset.labels])\n",
    "sampler = WeightedRandomSampler(samples_weights, len(samples_weights))\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "# 3. ëª¨ë¸ ì •ì˜ ë° ìš”ì•½í‘œ ì¶œë ¥ (ì¥ì¹˜ ë¶ˆì¼ì¹˜ ìˆ˜ì •)\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"     MODEL ARCHITECTURE\")\n",
    "print(\"=\"*30)\n",
    "# device ì¸ìì— ì§ì ‘ ìƒì„±í•œ device ê°ì²´ ì „ë‹¬\n",
    "summary(model, input_size=(3, 224, 224), device=device.type) \n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "# 4. í•™ìŠµ ì„¤ì •\n",
    "total_epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 5. ì‹œê°„ ì¸¡ì • í•™ìŠµ ë£¨í”„\n",
    "start_time = time.time()\n",
    "print(f\"í•™ìŠµ ì‹œì‘ ì‹œê°: {time.ctime(start_time)}\")\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # ì‹œê°„ ê³„ì‚°\n",
    "    epoch_end = time.time()\n",
    "    epoch_duration = epoch_end - epoch_start\n",
    "    total_elapsed = epoch_end - start_time\n",
    "    avg_epoch_time = total_elapsed / (epoch + 1)\n",
    "    remaining_epochs = total_epochs - (epoch + 1)\n",
    "    estimated_remaining_time = avg_epoch_time * remaining_epochs\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}] ì™„ë£Œ\")\n",
    "    print(f\" - ì†Œìš” ì‹œê°„: {timedelta(seconds=int(epoch_duration))}\")\n",
    "    print(f\" - ì´ ê²½ê³¼ ì‹œê°„: {timedelta(seconds=int(total_elapsed))}\")\n",
    "    print(f\" - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {timedelta(seconds=int(estimated_remaining_time))}\")\n",
    "    print(f\" - í‰ê·  Loss: {running_loss/len(loader):.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nìµœì¢… ì™„ë£Œ! ì´ ì†Œìš”: {timedelta(seconds=int(time.time() - start_time))}\")\n",
    "torch.save(model.state_dict(), \"dfdc_resnet18_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87398b2-53cf-4161-be5c-bb59363e4bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LAYER NAME                     PARAMETERS          \n",
      "==================================================\n",
      "conv1.weight                   9408                \n",
      "bn1.weight                     64                  \n",
      "bn1.bias                       64                  \n",
      "layer1.0.conv1.weight          36864               \n",
      "layer1.0.bn1.weight            64                  \n",
      "layer1.0.bn1.bias              64                  \n",
      "layer1.0.conv2.weight          36864               \n",
      "layer1.0.bn2.weight            64                  \n",
      "layer1.0.bn2.bias              64                  \n",
      "layer1.1.conv1.weight          36864               \n",
      "layer1.1.bn1.weight            64                  \n",
      "layer1.1.bn1.bias              64                  \n",
      "layer1.1.conv2.weight          36864               \n",
      "layer1.1.bn2.weight            64                  \n",
      "layer1.1.bn2.bias              64                  \n",
      "layer2.0.conv1.weight          73728               \n",
      "layer2.0.bn1.weight            128                 \n",
      "layer2.0.bn1.bias              128                 \n",
      "layer2.0.conv2.weight          147456              \n",
      "layer2.0.bn2.weight            128                 \n",
      "layer2.0.bn2.bias              128                 \n",
      "layer2.0.downsample.0.weight   8192                \n",
      "layer2.0.downsample.1.weight   128                 \n",
      "layer2.0.downsample.1.bias     128                 \n",
      "layer2.1.conv1.weight          147456              \n",
      "layer2.1.bn1.weight            128                 \n",
      "layer2.1.bn1.bias              128                 \n",
      "layer2.1.conv2.weight          147456              \n",
      "layer2.1.bn2.weight            128                 \n",
      "layer2.1.bn2.bias              128                 \n",
      "layer3.0.conv1.weight          294912              \n",
      "layer3.0.bn1.weight            256                 \n",
      "layer3.0.bn1.bias              256                 \n",
      "layer3.0.conv2.weight          589824              \n",
      "layer3.0.bn2.weight            256                 \n",
      "layer3.0.bn2.bias              256                 \n",
      "layer3.0.downsample.0.weight   32768               \n",
      "layer3.0.downsample.1.weight   256                 \n",
      "layer3.0.downsample.1.bias     256                 \n",
      "layer3.1.conv1.weight          589824              \n",
      "layer3.1.bn1.weight            256                 \n",
      "layer3.1.bn1.bias              256                 \n",
      "layer3.1.conv2.weight          589824              \n",
      "layer3.1.bn2.weight            256                 \n",
      "layer3.1.bn2.bias              256                 \n",
      "layer4.0.conv1.weight          1179648             \n",
      "layer4.0.bn1.weight            512                 \n",
      "layer4.0.bn1.bias              512                 \n",
      "layer4.0.conv2.weight          2359296             \n",
      "layer4.0.bn2.weight            512                 \n",
      "layer4.0.bn2.bias              512                 \n",
      "layer4.0.downsample.0.weight   131072              \n",
      "layer4.0.downsample.1.weight   512                 \n",
      "layer4.0.downsample.1.bias     512                 \n",
      "layer4.1.conv1.weight          2359296             \n",
      "layer4.1.bn1.weight            512                 \n",
      "layer4.1.bn1.bias              512                 \n",
      "layer4.1.conv2.weight          2359296             \n",
      "layer4.1.bn2.weight            512                 \n",
      "layer4.1.bn2.bias              512                 \n",
      "fc.weight                      512                 \n",
      "fc.bias                        1                   \n",
      "==================================================\n",
      "Total Trainable Params: 11,177,025\n",
      "==================================================\n",
      "\n",
      "í•™ìŠµ ì‹œì‘ ì‹œê°: Fri Jan 30 03:38:33 2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpjp/dfdc_env/lib/python3.12/site-packages/torch/nn/functional.py:3244: UserWarning: The operator 'aten::log_sigmoid_forward' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /__w/1/s/pytorch-directml-plugin/torch_directml/csrc/dml/dml_cpu_fallback.cpp:15.)\n",
      "  return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] ì™„ë£Œ\n",
      " - ì†Œìš” ì‹œê°„: 0:01:12\n",
      " - ì´ ê²½ê³¼ ì‹œê°„: 0:01:12\n",
      " - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: 0:10:55\n",
      " - í‰ê·  Loss: 0.6123\n",
      "----------------------------------------\n",
      "Epoch [2/10] ì™„ë£Œ\n",
      " - ì†Œìš” ì‹œê°„: 0:01:09\n",
      " - ì´ ê²½ê³¼ ì‹œê°„: 0:02:22\n",
      " - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: 0:09:28\n",
      " - í‰ê·  Loss: 0.4386\n",
      "----------------------------------------\n",
      "Epoch [3/10] ì™„ë£Œ\n",
      " - ì†Œìš” ì‹œê°„: 0:01:08\n",
      " - ì´ ê²½ê³¼ ì‹œê°„: 0:03:30\n",
      " - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: 0:08:10\n",
      " - í‰ê·  Loss: 0.3142\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. DirectML ì„¤ì •\n",
    "device = torch_directml.device()\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ ë° ê°€ì¤‘ì¹˜ ìƒ˜í”ŒëŸ¬\n",
    "class DFDCFinalDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        for label in ['REAL', 'FAKE']:\n",
    "            path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(path): continue\n",
    "            for img in os.listdir(path):\n",
    "                l_idx = 1 if label == 'FAKE' else 0\n",
    "                self.samples.append((os.path.join(path, img), l_idx))\n",
    "                self.labels.append(l_idx)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = DFDCFinalDataset('/mnt/c/Users/wlvud/dfdc_full_faces', transform=transform)\n",
    "\n",
    "class_counts = np.bincount(dataset.labels)\n",
    "weights = 1. / class_counts\n",
    "samples_weights = torch.from_numpy(weights[dataset.labels])\n",
    "sampler = WeightedRandomSampler(samples_weights, len(samples_weights))\n",
    "loader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "# 3. ëª¨ë¸ ì •ì˜ ë° ìš”ì•½ ì¶œë ¥ (PyTorch ë‚´ì¥ ë°©ì‹)\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"{'LAYER NAME':<30} {'PARAMETERS':<20}\")\n",
    "print(\"=\"*50)\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name:<30} {param.numel():<20}\")\n",
    "        total_params += param.numel()\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Trainable Params: {total_params:,}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# 4. í•™ìŠµ ì„¤ì •\n",
    "total_epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 5. ì‹œê°„ ì¸¡ì • í•™ìŠµ ë£¨í”„\n",
    "start_time = time.time()\n",
    "print(f\"í•™ìŠµ ì‹œì‘ ì‹œê°: {time.ctime(start_time)}\")\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start\n",
    "    total_elapsed = time.time() - start_time\n",
    "    avg_epoch_time = total_elapsed / (epoch + 1)\n",
    "    remaining_time = avg_epoch_time * (total_epochs - (epoch + 1))\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}] ì™„ë£Œ\")\n",
    "    print(f\" - ì†Œìš” ì‹œê°„: {timedelta(seconds=int(epoch_duration))}\")\n",
    "    print(f\" - ì´ ê²½ê³¼ ì‹œê°„: {timedelta(seconds=int(total_elapsed))}\")\n",
    "    print(f\" - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {timedelta(seconds=int(remaining_time))}\")\n",
    "    print(f\" - í‰ê·  Loss: {running_loss/len(loader):.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "torch.save(model.state_dict(), \"dfdc_resnet18_final.pth\")\n",
    "print(f\"ìµœì¢… ì™„ë£Œ! ì´ ì†Œìš”: {timedelta(seconds=int(time.time() - start_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab3c248-05b5-4869-82dc-cc241720dfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropped Escape call with ulEscapeCode : 0x03007703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°€ì† ìµœì í™” í•™ìŠµ ì‹œì‘ (CPU Fallback ì œê±° ì™„ë£Œ)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m model.train()\n\u001b[32m     36\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mloader\u001b[49m):\n\u001b[32m     39\u001b[39m     images, labels = images.to(device), labels.to(device)\n\u001b[32m     41\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[31mNameError\u001b[39m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "device = torch_directml.device()\n",
    "\n",
    "# [ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë° ì „ì²˜ë¦¬ëŠ” ì´ì „ê³¼ ë™ì¼]\n",
    "# ... (ìƒëµ: ì´ì „ ì½”ë“œì˜ DFDCFinalDataset ì‚¬ìš©)\n",
    "\n",
    "# 3. ëª¨ë¸ ì •ì˜ (Sigmoidë¥¼ ë‹¤ì‹œ ì¶”ê°€)\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 1),\n",
    "    nn.Sigmoid() # GPU ê°€ì†ì´ ì§€ì›ë˜ëŠ” Sigmoidë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# 4. ì†ì‹¤ í•¨ìˆ˜ ë³€ê²½ (CPU Fallbackì„ ìœ ë°œí•˜ëŠ” LogitsLoss ëŒ€ì‹  BCELoss ì‚¬ìš©)\n",
    "# BCELoss ì—°ì‚° ì‹œ DML ì§€ì›ì„ ìœ„í•´ ì•ˆì •ì ì¸ ìƒìˆ˜ë¥¼ ë”í•´ì£¼ëŠ” ìˆ˜ë™ ì—°ì‚° ì ìš© ê°€ëŠ¥ì„± ëŒ€ë¹„\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss() # ì´ í•¨ìˆ˜ëŠ” DMLì—ì„œ ê°€ì† ì§€ì›ë¨\n",
    "\n",
    "# 5. ì‹œê°„ ì¸¡ì • í•™ìŠµ ë£¨í”„\n",
    "start_time = time.time()\n",
    "print(f\"ê°€ì† ìµœì í™” í•™ìŠµ ì‹œì‘ (CPU Fallback ì œê±° ì™„ë£Œ)\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        # BCELoss ì—°ì‚° (DML ê°€ì† ì˜ì—­)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # ì‹œê°„ ê³„ì‚° ë¡œì§\n",
    "    epoch_duration = time.time() - epoch_start\n",
    "    total_elapsed = time.time() - start_time\n",
    "    avg_epoch_time = total_elapsed / (epoch + 1)\n",
    "    remaining_time = avg_epoch_time * (10 - (epoch + 1))\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/10] | ì†Œìš”: {timedelta(seconds=int(epoch_duration))} | ë‚¨ì€ì‹œê°„: {timedelta(seconds=int(remaining_time))}\")\n",
    "    print(f\" - í‰ê·  Loss: {running_loss/len(loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"dfdc_resnet18_fast.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea2f51c-d832-4f49-adba-b42230767c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê°€ì† ìµœì í™” í•™ìŠµ ì‹œì‘ (ì¥ì¹˜: privateuseone:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpjp/dfdc_env/lib/python3.12/site-packages/torch/nn/functional.py:3172: UserWarning: The operator 'aten::binary_cross_entropy' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /__w/1/s/pytorch-directml-plugin/torch_directml/csrc/dml/dml_cpu_fallback.cpp:15.)\n",
      "  return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] ì™„ë£Œ\n",
      " - ì†Œìš”: 0:01:12 | ë‚¨ì€: 0:10:53\n",
      " - í‰ê·  Loss: 0.6222\n",
      "---------------------------------------------\n",
      "Epoch [2/10] ì™„ë£Œ\n",
      " - ì†Œìš”: 0:01:07 | ë‚¨ì€: 0:09:20\n",
      " - í‰ê·  Loss: 0.4598\n",
      "---------------------------------------------\n",
      "Epoch [3/10] ì™„ë£Œ\n",
      " - ì†Œìš”: 0:01:08 | ë‚¨ì€: 0:08:07\n",
      " - í‰ê·  Loss: 0.2925\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     83\u001b[39m     loss = criterion(outputs, labels) \u001b[38;5;66;03m# GPU ê°€ì† ì—°ì‚°\u001b[39;00m\n\u001b[32m     84\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     running_loss += loss.item()\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# ì‹œê°„ ë¶„ì„\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    480\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    481\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    482\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    487\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     88\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     91\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/optim/adam.py:226\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    214\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    216\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    217\u001b[39m         group,\n\u001b[32m    218\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    223\u001b[39m         state_steps,\n\u001b[32m    224\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/optim/optimizer.py:161\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/optim/adam.py:766\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    764\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dfdc_env/lib/python3.12/site-packages/torch/optim/adam.py:534\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    529\u001b[39m         device_grads = torch._foreach_add(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    530\u001b[39m             device_grads, device_params, alpha=weight_decay\n\u001b[32m    531\u001b[39m         )\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m torch._foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[32m    537\u001b[39m torch._foreach_addcmul_(\n\u001b[32m    538\u001b[39m     device_exp_avg_sqs, device_grads, device_grads, \u001b[32m1\u001b[39m - beta2\n\u001b[32m    539\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. ì¥ì¹˜ ì„¤ì •\n",
    "device = torch_directml.device()\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "class DFDCFinalDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        if not os.path.exists(root_dir):\n",
    "            raise FileNotFoundError(f\"ê²½ë¡œ ì—†ìŒ: {root_dir}\")\n",
    "        for label in ['REAL', 'FAKE']:\n",
    "            path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(path): continue\n",
    "            for img in os.listdir(path):\n",
    "                l_idx = 1 if label == 'FAKE' else 0\n",
    "                self.samples.append((os.path.join(path, img), l_idx))\n",
    "                self.labels.append(l_idx)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "# 3. ì „ì²˜ë¦¬ ë° ë¡œë” ì„¤ì •\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = DFDCFinalDataset('/mnt/c/Users/wlvud/dfdc_full_faces', transform=transform)\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ìƒ˜í”Œë§ (ë¶ˆê· í˜• í•´ê²°)\n",
    "class_counts = np.bincount(dataset.labels)\n",
    "weights = 1. / class_counts\n",
    "samples_weights = torch.from_numpy(weights[dataset.labels])\n",
    "sampler = WeightedRandomSampler(samples_weights, len(samples_weights))\n",
    "loader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "# 4. ëª¨ë¸ êµ¬ì¡° ìµœì í™” (Sigmoid ì¶”ê°€ë¡œ GPU ê°€ì† ë³´ì¥)\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# 5. ìµœì í™” ë„êµ¬ (DML í˜¸í™˜ BCELoss)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 6. í•™ìŠµ ë£¨í”„ (ì‹œê°„ ì¸¡ì • í¬í•¨)\n",
    "total_epochs = 10\n",
    "start_time = time.time()\n",
    "print(f\"âœ… ê°€ì† ìµœì í™” í•™ìŠµ ì‹œì‘ (ì¥ì¹˜: {device})\")\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels) # GPU ê°€ì† ì—°ì‚°\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # ì‹œê°„ ë¶„ì„\n",
    "    epoch_duration = time.time() - epoch_start\n",
    "    total_elapsed = time.time() - start_time\n",
    "    avg_epoch_time = total_elapsed / (epoch + 1)\n",
    "    remaining_time = avg_epoch_time * (total_epochs - (epoch + 1))\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}] ì™„ë£Œ\")\n",
    "    print(f\" - ì†Œìš”: {timedelta(seconds=int(epoch_duration))} | ë‚¨ì€: {timedelta(seconds=int(remaining_time))}\")\n",
    "    print(f\" - í‰ê·  Loss: {running_loss/len(loader):.4f}\")\n",
    "    print(\"-\" * 45)\n",
    "\n",
    "torch.save(model.state_dict(), \"dfdc_resnet18_fast.pth\")\n",
    "print(f\"ğŸš€ ì „ì²´ í•™ìŠµ ì™„ë£Œ! íŒŒì¼ ì €ì¥: dfdc_resnet18_fast.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba70a4f-8406-466c-91ae-b8cf772623d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LAYER (ResNet18)               PARAMS              \n",
      "==================================================\n",
      "layer4.1.conv2.weight          2359296             \n",
      "fc.0.weight                    512                 \n",
      "fc.0.bias                      1                   \n",
      "... (skipped middle layers) ...\n",
      "--------------------------------------------------\n",
      "Total Trainable Params: 11,177,025\n",
      "==================================================\n",
      "\n",
      "â³ ì„±ëŠ¥ ì¸¡ì • ë° ì˜ˆìƒ ì‹œê°„ ê³„ì‚° ì¤‘ (Warm-up 30 Batches)...\n",
      "âœ… ë°°ì¹˜ë‹¹ í‰ê·  ì†Œìš” ì‹œê°„: 0.3366ì´ˆ\n",
      "ğŸš€ ì´ ì˜ˆìƒ ì†Œìš” ì‹œê°„: 0:09:05\n",
      "==================================================\n",
      "\n",
      "ğŸ”¥ í•™ìŠµ ì‹œì‘ (Total Epochs: 10)\n",
      "Epoch [1/10] | Loss: 0.6112 | Acc: 66.87% | Val Acc: 53.41%\n",
      "Epoch [2/10] | Loss: 0.4199 | Acc: 80.84% | Val Acc: 81.04%\n",
      "Epoch [3/10] | Loss: 0.2841 | Acc: 88.04% | Val Acc: 83.90%\n",
      "Epoch [4/10] | Loss: 0.1743 | Acc: 93.21% | Val Acc: 81.58%\n",
      "Epoch [5/10] | Loss: 0.1409 | Acc: 94.99% | Val Acc: 83.98%\n",
      "Epoch [6/10] | Loss: 0.0926 | Acc: 97.02% | Val Acc: 82.89%\n",
      "Epoch [7/10] | Loss: 0.0783 | Acc: 97.37% | Val Acc: 85.06%\n",
      "Epoch [8/10] | Loss: 0.0686 | Acc: 97.56% | Val Acc: 85.60%\n",
      "Epoch [9/10] | Loss: 0.0719 | Acc: 97.33% | Val Acc: 84.37%\n",
      "Epoch [10/10] | Loss: 0.0780 | Acc: 97.43% | Val Acc: 85.68%\n",
      "\n",
      "==================================================\n",
      "             FINAL REPORT             \n",
      "==================================================\n",
      "ì´ ì†Œìš” ì‹œê°„ : 0:10:23\n",
      "ìµœì¢… ì •í™•ë„ (Acc)     : 97.43%\n",
      "ìµœì¢… ê²€ì¦ ì •í™•ë„ (Val Acc): 85.68%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. ì„¤ì • ë° ì¥ì¹˜ ì¤€ë¹„\n",
    "device = torch_directml.device()\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (Split ê¸°ëŠ¥ ë‚´ì¥)\n",
    "class DFDCSplitDataset(Dataset):\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "        # ë¼ë²¨ ì¶”ì¶œ (Samplerìš©)\n",
    "        self.labels = [item[1] for item in self.data_list]\n",
    "\n",
    "    def __len__(self): return len(self.data_list)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data_list[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "def prepare_data(root_dir):\n",
    "    all_data = []\n",
    "    if not os.path.exists(root_dir): raise FileNotFoundError(f\"ê²½ë¡œ ì—†ìŒ: {root_dir}\")\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    for label_name in ['REAL', 'FAKE']:\n",
    "        path = os.path.join(root_dir, label_name)\n",
    "        if not os.path.exists(path): continue\n",
    "        for img in os.listdir(path):\n",
    "            l_idx = 1 if label_name == 'FAKE' else 0\n",
    "            all_data.append((os.path.join(path, img), l_idx))\n",
    "            \n",
    "    # 8:2 ë¶„í•  (Train/Val)\n",
    "    np.random.shuffle(all_data)\n",
    "    split_idx = int(len(all_data) * 0.8)\n",
    "    return all_data[:split_idx], all_data[split_idx:]\n",
    "\n",
    "# 3. ì „ì²˜ë¦¬ ë° ë¡œë” ìƒì„±\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_list, val_list = prepare_data('/mnt/c/Users/wlvud/dfdc_full_faces')\n",
    "train_dataset = DFDCSplitDataset(train_list, transform=transform)\n",
    "val_dataset = DFDCSplitDataset(val_list, transform=transform)\n",
    "\n",
    "# Sampler (Train set ë¶ˆê· í˜• í•´ê²°)\n",
    "class_counts = np.bincount(train_dataset.labels)\n",
    "weights = 1. / class_counts\n",
    "samples_weights = torch.from_numpy(weights[train_dataset.labels])\n",
    "sampler = WeightedRandomSampler(samples_weights, len(samples_weights))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 4. ëª¨ë¸ ì •ì˜ (Sigmoid + BCELoss ìµœì í™”)\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: ëª¨ë¸ ìš”ì•½ ì¶œë ¥\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"{'LAYER (ResNet18)':<30} {'PARAMS':<20}\")\n",
    "print(\"=\"*50)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" in name or \"layer4.1.conv2\" in name: # ì£¼ìš” ë ˆì´ì–´ë§Œ ì¶œë ¥\n",
    "        print(f\"{name:<30} {param.numel():<20}\")\n",
    "print(f\"... (skipped middle layers) ...\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Trainable Params: {total_params:,}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: ì›Œë°ì—… ë° ì˜ˆìƒ ì‹œê°„ ì‚°ì¶œ\n",
    "# ==========================================\n",
    "print(\"â³ ì„±ëŠ¥ ì¸¡ì • ë° ì˜ˆìƒ ì‹œê°„ ê³„ì‚° ì¤‘ (Warm-up 30 Batches)...\")\n",
    "model.train()\n",
    "warmup_start = time.time()\n",
    "warmup_batches = 30\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    if i >= warmup_batches: break\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "warmup_end = time.time()\n",
    "avg_batch_time = (warmup_end - warmup_start) / warmup_batches\n",
    "total_batches = len(train_loader) * EPOCHS\n",
    "estimated_total_time = avg_batch_time * total_batches\n",
    "\n",
    "print(f\"âœ… ë°°ì¹˜ë‹¹ í‰ê·  ì†Œìš” ì‹œê°„: {avg_batch_time:.4f}ì´ˆ\")\n",
    "print(f\"ğŸš€ ì´ ì˜ˆìƒ ì†Œìš” ì‹œê°„: {timedelta(seconds=int(estimated_total_time))}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: ë³¸ í•™ìŠµ ì‹œì‘\n",
    "# ==========================================\n",
    "print(f\"ğŸ”¥ í•™ìŠµ ì‹œì‘ (Total Epochs: {EPOCHS})\")\n",
    "real_start_time = time.time()\n",
    "\n",
    "final_train_acc = 0\n",
    "final_val_acc = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Train Loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    final_train_acc = train_acc # ë§ˆì§€ë§‰ ê°’ ì €ì¥\n",
    "    \n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            \n",
    "    val_acc = 100 * correct_val / total_val\n",
    "    final_val_acc = val_acc # ë§ˆì§€ë§‰ ê°’ ì €ì¥\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Loss: {running_loss/len(train_loader):.4f} | Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "# ==========================================\n",
    "total_duration = time.time() - real_start_time\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"             FINAL REPORT             \")\n",
    "print(\"=\"*50)\n",
    "print(f\"ì´ ì†Œìš” ì‹œê°„ : {timedelta(seconds=int(total_duration))}\")\n",
    "print(f\"ìµœì¢… ì •í™•ë„ (Acc)     : {final_train_acc:.2f}%\")\n",
    "print(f\"ìµœì¢… ê²€ì¦ ì •í™•ë„ (Val Acc): {final_val_acc:.2f}%\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "torch.save(model.state_dict(), \"dfdc_resnet18_complete.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0a9f34-13e2-4137-982c-4e75cbdc05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9b5332-e2ce-426a-b66b-291f048205fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëœë¤ ì„ íƒëœ íŒŒì¼: /mnt/c/Users/wlvud/dfdc_data/dfdc_train_part_07/dfdc_train_part_7/qckpcqitnd.mp4\n",
      "ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘: qckpcqitnd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_243882/133590216.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
      "pid:243882 tid:0x77e12482f080 [topology_sysfs_get_system_props] No WDDM adapters found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "ğŸ“Š ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œ\n",
      "========================================\n",
      "ê²€ì¶œëœ ì–¼êµ´ í”„ë ˆì„ ìˆ˜ : 60ê°œ\n",
      "ë”¥í˜ì´í¬ í™•ë¥         : 97.24%\n",
      "ê²°ê³¼ íŒì •            : ğŸš¨ FAKE (ì¡°ì‘ë¨)\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 1. ì„¤ì •\n",
    "device = torch_directml.device()\n",
    "MODEL_PATH = \"dfdc_resnet18_complete.pth\"\n",
    "FACE_CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "\n",
    "# 2. ëª¨ë¸ êµ¬ì¡° ì •ì˜ (í•™ìŠµë•Œì™€ 100% ë™ì¼í•´ì•¼ í•¨)\n",
    "def load_model():\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 3. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 4. ì˜ìƒ íŒë³„ í•¨ìˆ˜\n",
    "def predict_video(video_path):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘: {os.path.basename(video_path)}\")\n",
    "    \n",
    "    # ëª¨ë¸ & ì–¼êµ´ ì¸ì‹ê¸° ë¡œë“œ\n",
    "    model = load_model()\n",
    "    face_cascade = cv2.CascadeClassifier(FACE_CASCADE_PATH)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    fake_scores = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        frame_count += 1\n",
    "        # ì†ë„ë¥¼ ìœ„í•´ 5í”„ë ˆì„ë§ˆë‹¤ 1ë²ˆì”©ë§Œ ì •ë°€ ê²€ì‚¬\n",
    "        if frame_count % 5 != 0: continue\n",
    "            \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            # ì–¼êµ´ ì¶”ì¶œ\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            try:\n",
    "                # PIL Imageë¡œ ë³€í™˜\n",
    "                face_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "                # í…ì„œ ë³€í™˜ ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
    "                input_tensor = transform(face_pil).unsqueeze(0).to(device)\n",
    "                \n",
    "                # ì˜ˆì¸¡\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)\n",
    "                    score = output.item() # 0.0(REAL) ~ 1.0(FAKE)\n",
    "                    fake_scores.append(score)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    # ê²°ê³¼ ì§‘ê³„\n",
    "    if not fake_scores:\n",
    "        print(\"âš ï¸ ì–¼êµ´ì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    avg_score = sum(fake_scores) / len(fake_scores)\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"ğŸ“Š ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œ\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"ê²€ì¶œëœ ì–¼êµ´ í”„ë ˆì„ ìˆ˜ : {len(fake_scores)}ê°œ\")\n",
    "    print(f\"ë”¥í˜ì´í¬ í™•ë¥         : {avg_score*100:.2f}%\")\n",
    "    \n",
    "    if avg_score > 0.5:\n",
    "        print(\"ê²°ê³¼ íŒì •            : ğŸš¨ FAKE (ì¡°ì‘ë¨)\")\n",
    "    else:\n",
    "        print(\"ê²°ê³¼ íŒì •            : âœ… REAL (ì›ë³¸)\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "# ==========================================\n",
    "# ì‹¤í–‰ë¶€: ì—¬ê¸°ì— í…ŒìŠ¤íŠ¸í•  ì˜ìƒ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”\n",
    "# ==========================================\n",
    "# ì˜ˆì‹œ: ê¸°ì¡´ ë°ì´í„°ì…‹ ì¤‘ í•˜ë‚˜ë¥¼ ê³¨ë¼ ë„£ì–´ë³´ì„¸ìš”.\n",
    "target_video = \"/mnt/c/Users/wlvud/dfdc_data/dfdc_train_part_00/ì˜ìƒíŒŒì¼ëª….mp4\" \n",
    "\n",
    "# ê²½ë¡œë¥¼ ëª¨ë¥¼ ê²½ìš°, ì•„ë˜ ì½”ë“œë¥¼ ì£¼ì„ í•´ì œí•˜ì—¬ ë°ì´í„°ì…‹ ì¤‘ ëœë¤ìœ¼ë¡œ í•˜ë‚˜ë¥¼ ë½‘ì•„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "import random, glob\n",
    "files = glob.glob(\"/mnt/c/Users/wlvud/dfdc_data/**/*.mp4\", recursive=True)\n",
    "if files:\n",
    "   target_video = random.choice(files)\n",
    "   print(f\"ëœë¤ ì„ íƒëœ íŒŒì¼: {target_video}\")\n",
    "\n",
    "predict_video(target_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8c0c10-6884-4022-867f-b696ae1b256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#íŒë… ê²°ê³¼ ì •ë‹µ ê²€ì¦ (Fact Check)\n",
    "#ë°©ê¸ˆ í…ŒìŠ¤íŠ¸í•œ qckpcqitnd.mp4ê°€ ì‹¤ì œë¡œ ì¡°ì‘ëœ ì˜ìƒì¸ì§€, ì•„ë‹ˆë©´ ëª¨ë¸ì´ í‹€ë ¸ëŠ”ì§€ ë©”íƒ€ë°ì´í„°ë¥¼ ê¹Œì„œ í™•ì¸í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd253a6-260f-4314-a4eb-2e80ab376aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì •ë‹µ í™•ì¸ ì¤‘... (qckpcqitnd.mp4)\n",
      "\n",
      "==============================\n",
      "ğŸ“„ íŒŒì¼ëª…     : qckpcqitnd.mp4\n",
      "âœ… ì‹¤ì œ ì •ë‹µ   : FAKE\n",
      "ğŸ¤– ëª¨ë¸ ì˜ˆì¸¡   : FAKE (97.24%)\n",
      "------------------------------\n",
      "ê²°ê³¼: ğŸ‰ ì •ë‹µì…ë‹ˆë‹¤! (ëª¨ë¸ì´ ì¡°ì‘ì„ ì •í™•íˆ íƒì§€í•¨)\n",
      "ì°¸ê³ : ì´ ì˜ìƒì€ 'roykwzcnwf.mp4' ì˜ìƒì„ ì¡°ì‘í•œ ê²ƒì…ë‹ˆë‹¤.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ë°©ê¸ˆ í…ŒìŠ¤íŠ¸ëœ íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ ì„¤ì •\n",
    "json_path = \"/mnt/c/Users/wlvud/dfdc_data/dfdc_train_part_07/dfdc_train_part_7/metadata.json\"\n",
    "target_video = \"qckpcqitnd.mp4\"\n",
    "\n",
    "if not os.path.exists(json_path):\n",
    "    # ê²½ë¡œê°€ ë‹¤ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìƒìœ„ í´ë”ë„ ì²´í¬\n",
    "    json_path = \"/mnt/c/Users/wlvud/dfdc_data/dfdc_train_part_07/metadata.json\"\n",
    "\n",
    "print(f\"ğŸ” ì •ë‹µ í™•ì¸ ì¤‘... ({target_video})\")\n",
    "\n",
    "try:\n",
    "    with open(json_path, 'r') as f:\n",
    "        meta = json.load(f)\n",
    "        \n",
    "    if target_video in meta:\n",
    "        info = meta[target_video]\n",
    "        label = info['label']\n",
    "        original = info.get('original', 'ì—†ìŒ (ì›ë³¸ì„)')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(f\"ğŸ“„ íŒŒì¼ëª…     : {target_video}\")\n",
    "        print(f\"âœ… ì‹¤ì œ ì •ë‹µ   : {label}\")\n",
    "        print(f\"ğŸ¤– ëª¨ë¸ ì˜ˆì¸¡   : FAKE (97.24%)\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        if label == \"FAKE\":\n",
    "            print(\"ê²°ê³¼: ğŸ‰ ì •ë‹µì…ë‹ˆë‹¤! (ëª¨ë¸ì´ ì¡°ì‘ì„ ì •í™•íˆ íƒì§€í•¨)\")\n",
    "            print(f\"ì°¸ê³ : ì´ ì˜ìƒì€ '{original}' ì˜ìƒì„ ì¡°ì‘í•œ ê²ƒì…ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(\"ê²°ê³¼: âŒ ì˜¤ë‹µì…ë‹ˆë‹¤. (ì›ë³¸ì„ ì¡°ì‘ìœ¼ë¡œ ì°©ê°í•¨)\")\n",
    "        print(\"=\"*30)\n",
    "    else:\n",
    "        print(\"âš ï¸ ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c2aec31-5bf0-4f33-9812-2ec729a2c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ ê³ ë„í™” (Albumentations ì ìš©)\n",
    "# í˜„ì¬ ë†’ì€ í›ˆë ¨ ì •í™•ë„ ëŒ€ë¹„ ë‚®ì€ ê²€ì¦ ì •í™•ë„ëŠ” ê³¼ì í•©(Overfitting) ì§•í›„ì…ë‹ˆë‹¤. ë”¥í˜ì´í¬ íŠ¹ìœ ì˜ ì••ì¶• ì†ì‹¤ê³¼ ë…¸ì´ì¦ˆì— ê°•ì¸í•´ì§€ë„ë¡ ë‹¨ìˆœ ë³€í˜•ì´ ì•„ë‹Œ ë„ë©”ì¸ íŠ¹í™” ì¦ê°•ì„ ì ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941cb2f9-261d-4d47-a040-90cac0d50060",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'albumentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mA\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ë”¥í˜ì´í¬ íƒì§€ ìµœì í™” ì¦ê°• êµ¬ì„±\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'albumentations'"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ë”¥í˜ì´í¬ íƒì§€ ìµœì í™” ì¦ê°• êµ¬ì„±\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    # ì••ì¶• ë…¸ì´ì¦ˆ ì¬í˜„ (Deepfakeì˜ í•µì‹¬ íŠ¹ì§• íŒŒê´´ ë°©ì§€)\n",
    "    A.OneOf([\n",
    "        A.ImageCompression(quality_lower=60, quality_upper=100, p=0.5),\n",
    "        A.GaussNoise(p=0.5),\n",
    "    ], p=0.3),\n",
    "    # ì¡°ëª… ë° ìƒ‰ìƒ ë³€í˜•\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.HueSaturationValue(p=0.2),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf73d47-fd05-4674-80af-cdcedf07bea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Downloading albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /home/jpjp/dfdc_env/lib/python3.12/site-packages (from albumentations) (2.4.1)\n",
      "Collecting scipy>=1.10.0 (from albumentations)\n",
      "  Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /home/jpjp/dfdc_env/lib/python3.12/site-packages (from albumentations) (6.0.3)\n",
      "Collecting pydantic>=2.9.2 (from albumentations)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting albucore==0.0.24 (from albumentations)\n",
      "  Downloading albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Downloading opencv_python_headless-4.13.0.90-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
      "  Downloading stringzilla-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
      "  Downloading simsimd-6.5.12-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (70 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/jpjp/dfdc_env/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m369.4/369.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading albucore-0.0.24-py3-none-any.whl (15 kB)\n",
      "Downloading opencv_python_headless-4.13.0.90-cp37-abi3-manylinux_2_28_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m463.6/463.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading simsimd-6.5.12-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (582 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m583.0/583.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading stringzilla-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: simsimd, typing-inspection, stringzilla, scipy, pydantic-core, opencv-python-headless, annotated-types, pydantic, albucore, albumentations\n",
      "Successfully installed albucore-0.0.24 albumentations-2.0.8 annotated-types-0.7.0 opencv-python-headless-4.13.0.90 pydantic-2.12.5 pydantic-core-2.41.5 scipy-1.17.0 simsimd-6.5.12 stringzilla-4.6.0 typing-inspection-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c87d681-3fab-4286-a708-2b00b91bd72e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DFDC)",
   "language": "python",
   "name": "dfdc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
